{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepLearning NLP Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Types of Natural Language Processing\n",
    "2. Classical vs Deep Learning Models\n",
    "3. End to end DL models\n",
    "4. Bag-of-words\n",
    "5. Seq2Seq Architecture\n",
    "6. Seq2Seq Training\n",
    "7. Beam Search Decoding\n",
    "8. Attention Mechanisms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of NLP:\n",
    "<b>fig types of NLP</b>\n",
    "![Screenshot%20from%202018-08-27%2021-40-24.png](Screenshot%20from%202018-08-27%2021-40-24.png)\n",
    "### Classical vs Deep Learning Models:\n",
    "Classical---> if/else Rules(chatbot),audio frequency components analysis for speech recognition(the sample is compared with pre recorded and recognised ones to determine what it is.),Bag of words model,<br>\n",
    "DeepNLP---> CNN for text recognition,<br>\n",
    "Seq2Seq ---> lots of applications<br>\n",
    "\n",
    "\n",
    "\n",
    "### End to end DL models:\n",
    "\n",
    "`for example` the customer service: human response---> dial pad response---> DNLP at recieving end and then DNLP at response end---> different neural nets involved will be trained on different data and hence will most probably not correspond each other.<br>\n",
    "End to end deeplearning model will like Seq2Seq take care of all these in one neural network to take the query and give the response.<br>\n",
    "<b>fig end to end deep learning model</b>\n",
    "![Screenshot%20from%202018-08-27%2022-32-46.png](Screenshot%20from%202018-08-27%2022-32-46.png)\n",
    "### Bag-of-words:\n",
    "Average number of words used by normal speaker---> 20000 and thus vector of 20000 0's is initialized start of start of sentence end of sentence and special words.<br>\n",
    "<b>fig: bag of words</b> \n",
    "![Screenshot%20from%202018-08-27%2022-40-02.png](Screenshot%20from%202018-08-27%2022-40-02.png)\n",
    "<br>\n",
    "after feeding our input the vector changes.<br>\n",
    "\n",
    "\n",
    "`Example` yes no response by training on elements to come up with response for different emails.<br>\n",
    "\n",
    "\n",
    "after getting the vectors, we apply a model for example---> logistic regression. what is likely to have response `yes` and what would be good to get response `No`<br>\n",
    "Then we feed our sentence into it before we get the logistic answer to our new email.<br>\n",
    "\n",
    "Again above can be implemented by `Neural network`<br>\n",
    "\n",
    "\n",
    "The bag of words though has its own limitations.for example the responses will be very trivial.<br>\n",
    "other limitations of bag of words are: limited input, doesn't take word into account, fixed sized output<br><p>\n",
    "    \n",
    "### Seq2Seq Architecture:\n",
    "To deal with the limitaitions of Bag of words, we deploy `RNNs`.<br>\n",
    "<b>fig seq2seq and every box is whole layer of RNN</b>\n",
    "![Screenshot%20from%202018-08-27%2022-49-29.png](Screenshot%20from%202018-08-27%2022-49-29.png)\n",
    "\n",
    "we use many to many neural network architecture.<br>\n",
    "\n",
    "here we create the vector with number of words directly into it. <br>\n",
    "We feed the values of vector in RNNs <br>\n",
    "<b>fig working of seq2seq architecture.</b>\n",
    "![Screenshot%20from%202018-08-27%2023-00-39.png](Screenshot%20from%202018-08-27%2023-00-39.png)\n",
    "\n",
    "The neural net will give out the probability score to the words, and then pick the one with highest probability.<br>\n",
    "Once the response is ready it gives you the output.<br>\n",
    "Hence it works as 2 parts: Encoders and decoders, one to train the model and then other to guess the output.<br>\n",
    "<b>fig seq2seq full encoding and decoding with complex structure to fully understand the human level conversation and give the non trivial response.</b>\n",
    "![Screenshot%20from%202018-08-27%2023-02-40.png](Screenshot%20from%202018-08-27%2023-02-40.png)\n",
    "### Seq2seq training:\n",
    "For training, we use actual labeled emails(i.e with response)<br>\n",
    "<b>fig of seq2seq training with all the weights in the network</b>\n",
    "![Screenshot%20from%202018-08-28%2009-28-58.png](Screenshot%20from%202018-08-28%2009-28-58.png)\n",
    "Given the certain email it will maximize the probability of probable words on every step, and backpropagate the errors in the network to adjust the weights so that the probability of these words from 20000 other words be higher for it to rise for the part of response.<br>\n",
    "\n",
    "![Screenshot%20from%202018-08-28%2009-49-48.png](Screenshot%20from%202018-08-28%2009-49-48.png)\n",
    "\n",
    "### Beam Search Decoding:\n",
    "Greedy Decoding and Beam search decoding to get the output from the Seq2Seq network.<br>\n",
    "\n",
    "\n",
    "#### Greedy Decoding:\n",
    "input---> socores ----> pick the one with highest score---> feed back----> scores ----> pick one with highest score----> so on:<br>\n",
    "Greedy as an it only picks the highest response with highest probability every time.<br>\n",
    "\n",
    "<b>fig of greedy decoding</b>\n",
    "![Screenshot%20from%202018-08-28%2010-06-28.png](Screenshot%20from%202018-08-28%2010-06-28.png)\n",
    "In the beam search however we decide the number of beams in every step----> the top x number of probable words are taken into consideration.<br>\n",
    "Then every beam progresses independently.i.e the errors are backpropagated in the network and then another word for that beam is selected accordingly.<br>\n",
    "\n",
    "The joint probability of each beam is calculated to pick the winner among the beams to come up with a response.<br>\n",
    "\n",
    "Number of beams increase exponentially, so to keep that in control, number of beams are truncated---> The ones with joint probability lower than that of certain threshold.<br>\n",
    "<b>Fig. Beam Search Decoding</b>\n",
    "\n",
    "\n",
    "### Attention Mechanisms:\n",
    "Encoder Decoder way of coming up with a response is trivial as at the EOS only we can start to decode, and we don't have luxury to look back. Hence these kinds can only be used to come up with responses for short sentences.<br>\n",
    "<p>\n",
    "    \n",
    "Through training, the words are given certain importance, for the response to be generated. These weights are used to come up with the context vector(taking weights and multiplying them with corresponding vector and then taking the sum of all). This context vector is fed into new layer into our decoder which comes up with the response.<br>\n",
    "<b>Fig Attention mechanism </b>\n",
    "\n",
    "Here in above figure to comeup with the pronoun `she` from `sister` the high weight on sister is very important here.<br>\n",
    "\n",
    "<b>Fig Example of attention mechanism while neural network was doing a translation, the white lighter parts are where the neural network was paying attention to while doing the translation.</b>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a chatbot using Cornel Movie corpus dataset.\n",
    "1. Creating an environment\n",
    "2. Data Preprocessing\n",
    "3. Building the Seq2Seq model\n",
    "4. Training the Seq2Seq model\n",
    "5. Testing the Seq2Seq model\n",
    "\n",
    "<p>\n",
    "    \n",
    "The dataset has two files: one having the lines from the movies and other denoting index of lines with some conversations.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Phase:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
